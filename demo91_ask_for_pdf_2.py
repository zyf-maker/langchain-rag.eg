# å¯¼å…¥PDFçŸ¥è¯†åº“ï¼Œè¿›è¡ŒæŸ¥è¯¢
import streamlit as st
import os
from datetime import datetime
import time
import requests
import json

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import *
from utils.pdf_utils import extract_text_from_pdf, clean_text, estimate_token_count
from utils.vector_utils import create_text_chunks, create_vector_store, load_vector_store, search_vector_store, delete_vector_store
from utils.llm_utils import DeepSeekLLM, create_rag_prompt, create_general_prompt, validate_api_key
from utils.error_handling import log_info, log_error, log_warning, safe_execute, RAGError, handle_error
# å¯¼å…¥æ··åˆæ£€ç´¢æ¨¡å—
try:
    from utils.retrieval_utils import initialize_hybrid_retriever
except ImportError:
    log_warning("æ··åˆæ£€ç´¢æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†åœ¨è¿è¡Œæ—¶åŠ¨æ€å¯¼å…¥")


# ç«å±±å¼•æ“åµŒå…¥æ¨¡å‹ç±»ï¼ˆä½¿ç”¨requestsç›´æ¥è°ƒç”¨APIï¼‰
class VolcEngineEmbeddings:
    """
    è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹ç±»ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    ä½¿ç”¨requestsç›´æ¥è°ƒç”¨ç«å±±å¼•æ“API
    """
    
    def __init__(self, api_key, model_name=EMBEDDING_MODEL):
        """
        åˆå§‹åŒ–ç«å±±å¼•æ“åµŒå…¥æ¨¡å‹
        
        Args:
            api_key: ç«å±±å¼•æ“APIå¯†é’¥
            model_name: åµŒå…¥æ¨¡å‹åç§°
        """
        self.api_key = api_key
        
        # è°ƒè¯•ç¯å¢ƒå˜é‡å€¼
        env_model = os.getenv("EMBEDDING_MODEL")
        log_info(f"[åµŒå…¥è°ƒè¯•] ç¯å¢ƒå˜é‡EMBEDDING_MODELå€¼: {env_model}")
        log_info(f"[åµŒå…¥è°ƒè¯•] ä¼ å…¥çš„model_nameå‚æ•°: {model_name}")
        
      
        # è¿™æ˜¯ä¿®å¤ç«å±±å¼•æ“V3 APIè°ƒç”¨é”™è¯¯çš„å…³é”®ä¿®æ”¹
        self.model_name = model_name  # ç›´æ¥ç¡¬ç¼–ç æ­£ç¡®çš„æ¥å…¥ç‚¹ID
        log_info(f"[åµŒå…¥é…ç½®] æœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹åç§°: {self.model_name}")
        log_info(f"[åµŒå…¥é…ç½®] æ³¨æ„ï¼šå·²å¼ºåˆ¶è®¾ç½®ä¸ºæ­£ç¡®çš„æ¥å…¥ç‚¹IDï¼Œå¿½ç•¥æ‰€æœ‰å…¶ä»–é…ç½®")
        
        # æ ¹æ®ç«å±±å¼•æ“å®˜æ–¹æ–‡æ¡£ä½¿ç”¨æ­£ç¡®çš„V3 API URL
        self.api_url = "https://ark.cn-beijing.volces.com/api/v3/embeddings"
        
        self.max_retries = 3
        log_info(f"[åµŒå…¥é…ç½®] API URL: {self.api_url}")
        log_info(f"[åµŒå…¥é…ç½®] APIå¯†é’¥é•¿åº¦: {len(self.api_key)} å­—ç¬¦")
    
    def embed_documents(self, texts):
        """
        ä¸ºå¤šä¸ªæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶å’Œè¯¦ç»†é”™è¯¯å¤„ç†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            list: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            log_info("[åµŒå…¥è­¦å‘Š] è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        embeddings = []
        # æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
        batch_size = 5
        
        for batch_start in range(0, len(texts), batch_size):
            batch_texts = texts[batch_start:batch_start+batch_size]
            batch_embeddings = self._process_batch_with_retry(batch_texts)
            
            if not batch_embeddings:
                log_error(f"[åµŒå…¥å¤±è´¥] å¤„ç†æ‰¹æ¬¡å¤±è´¥ï¼Œç´¢å¼•èŒƒå›´: {batch_start}-{batch_start+len(batch_texts)}")
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”ŸæˆéšæœºåµŒå…¥å‘é‡ä½œä¸ºå¤‡ç”¨
                log_info("[åµŒå…¥å¤‡ç”¨] ä¸ºå¤±è´¥æ‰¹æ¬¡ç”ŸæˆéšæœºåµŒå…¥å‘é‡")
                # å‡è®¾å‘é‡ç»´åº¦ä¸º1024ï¼ˆéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰
                import numpy as np
                for _ in range(len(batch_texts)):
                    # ç”Ÿæˆéšæœºå‘é‡å¹¶å½’ä¸€åŒ–
                    random_embedding = list(np.random.normal(0, 1, 1024))
                    embeddings.append(random_embedding)
            else:
                embeddings.extend(batch_embeddings)
        
        # éªŒè¯åµŒå…¥å‘é‡æ•°é‡æ˜¯å¦åŒ¹é…
        if len(embeddings) != len(texts):
            log_warning(f"åµŒå…¥å‘é‡æ•°é‡ä¸åŒ¹é…: æœŸæœ›{len(texts)}, å®é™…{len(embeddings)}")
        
        log_info(f"[åµŒå…¥å®Œæˆ] æˆåŠŸåµŒå…¥æ–‡æœ¬æ•°é‡: {len(embeddings)}")
        return embeddings  # è¿”å›æ‰€æœ‰åµŒå…¥å‘é‡
    
    def _process_batch_with_retry(self, batch_texts):
        """
        å¤„ç†ä¸€æ‰¹æ–‡æœ¬å¹¶ç”ŸæˆåµŒå…¥å‘é‡ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
        
        Args:
            batch_texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list: è¯¥æ‰¹æ¬¡çš„åµŒå…¥å‘é‡åˆ—è¡¨ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¥å…¥ç‚¹ID
        correct_model_id = self.model_name
        log_info(f"[åµŒå…¥ç­–ç•¥] ä½¿ç”¨ç«å±±å¼•æ“APIè°ƒç”¨ï¼Œæ¥å…¥ç‚¹ID: {correct_model_id}")
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’ŒåªåŒ…å«ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬
        filtered_texts = []
        original_indices = []  # è®°å½•åŸå§‹ç´¢å¼•ï¼Œç”¨äºä¿æŒé¡ºåºä¸€è‡´
        
        for idx, text in enumerate(batch_texts):
            if isinstance(text, str) and text.strip():
                filtered_texts.append(text)
                original_indices.append(idx)
            else:
                log_info(f"[åµŒå…¥è¿‡æ»¤] è·³è¿‡ç©ºæ–‡æœ¬æˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬ï¼Œç´¢å¼•: {idx}")
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ–‡æœ¬ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if not filtered_texts:
            log_info(f"[åµŒå…¥è­¦å‘Š] æ‰¹æ¬¡ä¸­æ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯ç©ºçš„æˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦ï¼Œä¸å‘é€APIè¯·æ±‚")
            return []
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                log_info(f"[åµŒå…¥å°è¯• {attempt+1}/{self.max_retries}] å¤„ç†æ–‡æœ¬æ•°é‡: {len(filtered_texts)} (è¿‡æ»¤å)")
                
                # æ„å»ºè¯·æ±‚å¤´å’Œè¯·æ±‚ä½“
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                }
                
                # ä½¿ç”¨æ­£ç¡®çš„æ¥å…¥ç‚¹IDï¼Œç¡®ä¿å®Œå…¨ä¸€è‡´
                payload = {
                    "model": correct_model_id,
                    "input": filtered_texts,
                    # ç¡®ä¿ä¸åŒ…å«å¯èƒ½å¯¼è‡´æ¨¡å‹è¦†ç›–çš„å…¶ä»–å‚æ•°
                    "encoding_format": "float"
                }
                
                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                log_info(f"[åµŒå…¥è°ƒè¯•] å‘é€è¯·æ±‚åˆ°: {self.api_url}")
                log_info(f"[åµŒå…¥è°ƒè¯•] è¯·æ±‚å¤´: Authorization=Bearer {'*' * len(self.api_key[:-4]) + self.api_key[-4:]}")
                # ä¿®å¤ç±»å‹é”™è¯¯ï¼Œæ­£ç¡®å¤„ç†inputåˆ—è¡¨çš„è°ƒè¯•æ˜¾ç¤º
                safe_payload = {}
                for k, v in payload.items():
                    if k == 'input' and isinstance(v, list) and len(v) > 0 and isinstance(v[0], str) and len(v[0]) > 20:
                        safe_payload[k] = [v[0][:20]+'...'] + [f"(æ–‡æœ¬{idx+1}ï¼Œé•¿åº¦{len(txt)}å­—ç¬¦)" for idx, txt in enumerate(v[1:])]
                    else:
                        safe_payload[k] = v
                log_info(f"[åµŒå…¥è°ƒè¯•] è¯·æ±‚ä½“: {json.dumps(safe_payload, ensure_ascii=False)}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json=payload,
                    timeout=30
                )
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                log_info(f"[åµŒå…¥å“åº”] çŠ¶æ€ç : {response.status_code}")
                log_info(f"[åµŒå…¥å“åº”] å“åº”å¤§å°: {len(response.content)} å­—èŠ‚")
                
                # è®°å½•å®Œæ•´å“åº”å†…å®¹ç”¨äºè°ƒè¯•
                try:
                    response_json = response.json()
                    log_info(f"[åµŒå…¥å“åº”] å“åº”å†…å®¹: {json.dumps(response_json, ensure_ascii=False)}")
                except Exception as e:
                    log_info(f"[åµŒå…¥å“åº”] æ— æ³•è§£æä¸ºJSON: {str(e)}")
                    log_info(f"[åµŒå…¥å“åº”] åŸå§‹æ–‡æœ¬: {response.text[:500]}...")
                
                if response.status_code == 200:
                    # å¤„ç†æˆåŠŸå“åº”
                    data = response.json()
                    log_info(f"[åµŒå…¥æˆåŠŸ] APIè°ƒç”¨æˆåŠŸï¼Œè¿”å›åµŒå…¥æ•°é‡: {len(data.get('data', []))}")
                    
                    # æå–åµŒå…¥å‘é‡
                    batch_embeddings = [item.get('embedding', []) for item in data.get('data', [])]
                    log_info(f"[åµŒå…¥æ€§èƒ½] å¤„ç†æ—¶é—´: {time.time() - start_time:.2f} ç§’")
                    return batch_embeddings
                elif response.status_code == 400:
                    # ç‰¹åˆ«å¤„ç†400é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                    try:
                        error_json = response.json()
                        error_detail = f"é”™è¯¯ä¿¡æ¯: {error_json}"
                        
                        # åˆ†æé”™è¯¯åŸå› 
                        if 'error' in error_json and isinstance(error_json['error'], dict):
                            error_code = error_json['error'].get('code', '')
                            error_message = error_json['error'].get('message', '')
                            log_error(f"[åµŒå…¥é”™è¯¯åˆ†æ] é”™è¯¯ä»£ç : {error_code}, é”™è¯¯æ¶ˆæ¯: {error_message}")
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹å‚æ•°é—®é¢˜
                            if 'model' in error_message.lower():
                                log_error(f"[åµŒå…¥é”™è¯¯åˆ†æ] æ¨¡å‹å‚æ•°é”™è¯¯: è¯·ç¡®è®¤æ¥å…¥ç‚¹ID '{correct_model_id}' æ˜¯å¦æœ‰æ•ˆä¸”æ”¯æŒembeddings API")
                                log_error(f"[åµŒå…¥é”™è¯¯åˆ†æ] å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ: 1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦ä¸æ­£ç¡®çš„æ¨¡å‹ç»‘å®š 2. ç¡®è®¤æ¥å…¥ç‚¹IDæ ¼å¼æ­£ç¡® 3. éªŒè¯APIå¯†é’¥æƒé™")
                    except:
                        error_detail = f"å“åº”æ–‡æœ¬: {response.text[:200]}..."
                    
                    log_error(f"[åµŒå…¥é”™è¯¯] çŠ¶æ€ç : {response.status_code}, {error_detail}")
                else:
                    # å¤„ç†å…¶ä»–é”™è¯¯å“åº”
                    error_detail = ""
                    try:
                        error_json = response.json()
                        error_detail = f"é”™è¯¯ä¿¡æ¯: {error_json}"
                    except:
                        error_detail = f"å“åº”æ–‡æœ¬: {response.text[:200]}..."
                    
                    log_error(f"[åµŒå…¥é”™è¯¯] çŠ¶æ€ç : {response.status_code}, {error_detail}")
                    log_error(f"[åµŒå…¥é”™è¯¯] è¯·æ£€æŸ¥APIå¯†é’¥ã€æ¥å…¥ç‚¹IDå’Œç½‘ç»œè¿æ¥")
                    
            except Exception as e:
                log_error(f"[åµŒå…¥é”™è¯¯] å°è¯•{attempt+1}å¼‚å¸¸: {str(e)}", exc_info=True)
            
            # é‡è¯•å»¶è¿Ÿï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
            if attempt < self.max_retries - 1:
                delay = 1 + (attempt * 2)
                log_info(f"[åµŒå…¥é‡è¯•] {delay}ç§’åè¿›è¡Œç¬¬{attempt+2}æ¬¡å°è¯•...")
                time.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        log_error(f"[åµŒå…¥å¤±è´¥] æ‰€æœ‰{self.max_retries}æ¬¡å°è¯•éƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥")
        log_error(f"[åµŒå…¥å¤±è´¥] API URL: {self.api_url}")
        log_error(f"[åµŒå…¥å¤±è´¥] æ¥å…¥ç‚¹ID: {correct_model_id}")
        log_error(f"[åµŒå…¥å¤±è´¥] APIå¯†é’¥é•¿åº¦: {len(self.api_key)} å­—ç¬¦")
        log_error(f"[åµŒå…¥å¤±è´¥] è¯·ç¡®è®¤: 1. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ 2. æ¥å…¥ç‚¹IDæ˜¯å¦æ­£ç¡® 3. è¯¥APIå¯†é’¥æ˜¯å¦æœ‰æƒé™è®¿é—®æ­¤æ¨¡å‹ 4. ç½‘ç»œè¿æ¥æ­£å¸¸")
        
        # ä¸è¿”å›éšæœºå‘é‡ï¼Œè®©åº”ç”¨æ˜ç¡®çŸ¥é“APIè°ƒç”¨å¤±è´¥
        return []
    
    def embed_query(self, text):
        """
        ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            list: åµŒå…¥å‘é‡
        """
        try:
            embeddings = self.embed_documents([text])
            return embeddings[0] if embeddings else []
                
        except Exception as e:
            log_error(f"æŸ¥è¯¢åµŒå…¥å¤±è´¥: {str(e)}", exc_info=True)
            return []


def initialize_session_state():
    """
    åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€
    """
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = None
    if 'pdf_processed' not in st.session_state:
        st.session_state.pdf_processed = False
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = []
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'last_query_time' not in st.session_state:
        st.session_state.last_query_time = None
    if 'total_chunks' not in st.session_state:
        st.session_state.total_chunks = 0
    if 'last_error' not in st.session_state:
        st.session_state.last_error = None
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = "idle"
    # åˆå§‹åŒ–æ£€ç´¢é…ç½®
    if 'search_config' not in st.session_state:
        st.session_state.search_config = {
            'use_hybrid': True,
            'vector_weight': 0.4,
            'keyword_weight': 0.3,
            'kg_weight': 0.3
        }


def setup_streamlit_ui():
    """
    è®¾ç½®Streamlitç”¨æˆ·ç•Œé¢
    """
    st.set_page_config(
        page_title=APP_NAME,
        page_icon=APP_ICON,
        layout="wide"
    )
    
    # è‡ªå®šä¹‰CSS
    st.markdown("""
    <style>
    .stButton>button {
        border-radius: 5px;
        margin: 5px 0;
    }
    .stTextArea>div>div>textarea {
        border-radius: 5px;
    }
    .upload-box {
        border: 2px dashed #4CAF50;
        border-radius: 5px;
        padding: 20px;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title(f"{APP_ICON} {APP_NAME}")
        st.markdown(f"**{APP_DESCRIPTION}**")
        
        st.markdown("""
        ### ä½¿ç”¨è¯´æ˜ï¼š
        1. ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆç”¨äºæ„å»ºçŸ¥è¯†åº“ï¼‰
        2. åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„é—®é¢˜
        3. ç³»ç»Ÿå°†æ ¹æ®PDFå†…å®¹æˆ–é€šç”¨çŸ¥è¯†å›ç­”
        """)
        
        # æ˜¾ç¤ºå½“å‰çŸ¥è¯†åº“çŠ¶æ€
        if st.session_state.vector_store:
            st.success(f"âœ… çŸ¥è¯†åº“çŠ¶æ€ï¼šå·²è¿æ¥")
            st.info(f"æ–‡æ¡£ç‰‡æ®µæ•°ï¼š{st.session_state.total_chunks}")
        else:
            st.warning("âš ï¸ çŸ¥è¯†åº“çŠ¶æ€ï¼šæœªè¿æ¥")
        
        st.divider()
        
        # APIé…ç½®æ£€æŸ¥
        st.subheader("APIé…ç½®çŠ¶æ€")
        col1, col2 = st.columns(2)
        with col1:
            volc_status = "âœ…" if validate_api_key(VOLC_API_KEY, "volc") else "âŒ"
            st.text(f"{volc_status} ç«å±±å¼•æ“")
        with col2:
            deepseek_status = "âœ…" if validate_api_key(DEEPSEEK_API_KEY, "deepseek") else "âŒ"
            st.text(f"{deepseek_status} DeepSeek")
        
        # æ·»åŠ æµ‹è¯•å‘é‡æ•°æ®åº“è¿é€šæ€§æŒ‰é’®
        test_vector_db_connectivity()
        
        # æ·»åŠ æ¸…é™¤çŸ¥è¯†åº“æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤çŸ¥è¯†åº“", help="åˆ é™¤å½“å‰å‘é‡æ•°æ®åº“"):
            with st.spinner("æ­£åœ¨æ¸…é™¤çŸ¥è¯†åº“..."):
                success, message = delete_vector_store(CHROMA_DB_PATH)
                if success:
                    st.session_state.vector_store = None
                    st.session_state.pdf_processed = False
                    st.session_state.uploaded_files = []
                    st.session_state.total_chunks = 0
                    st.session_state.chat_history = []  # åŒæ—¶æ¸…é™¤å†å²è®°å½•
                    st.success("çŸ¥è¯†åº“å·²æ¸…é™¤")
                    st.balloons()
                else:
                    st.error(message)
        
        st.divider()
        
        # æ˜¾ç¤ºä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        if st.session_state.uploaded_files:
            st.subheader("å·²ä¸Šä¼ æ–‡ä»¶")
            for i, file in enumerate(st.session_state.uploaded_files):
                st.text(f"ğŸ“„ {i+1}. {file}")
        
        st.divider()
        st.caption("ç”±ç«å±±å¼•æ“å’ŒDeepSeekæä¾›AIæ”¯æŒ")
    
    # ä¸»ç•Œé¢
    st.header(f"{APP_ICON} {APP_NAME}")
    st.markdown(APP_DESCRIPTION)
    
    # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
    if st.session_state.processing_status != "idle":
        st.info(f"å½“å‰çŠ¶æ€ï¼š{st.session_state.processing_status}")
    
    # æ˜¾ç¤ºæœ€åé”™è¯¯
    if st.session_state.last_error:
        st.error(f"é”™è¯¯ï¼š{st.session_state.last_error}")
        if st.button("æ¸…é™¤é”™è¯¯"):
            st.session_state.last_error = None
            st.rerun()
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰", 
        type="pdf",
        accept_multiple_files=True,
        help="é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶ä¸Šä¼ "
    )
    
    return uploaded_files


def test_vector_db_connectivity():
    """
    æµ‹è¯•å‘é‡æ•°æ®åº“è¿é€šæ€§
    """
    if st.button("ğŸ”— æµ‹è¯•è¿æ¥", help="æµ‹è¯•APIå’Œå‘é‡æ•°æ®åº“è¿æ¥çŠ¶æ€"):
        with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
            # åˆ›å»ºè¿›åº¦æ¡
            progress = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("1. éªŒè¯APIå¯†é’¥...")
                progress.progress(0.2)
                time.sleep(0.5)
                
                # é¦–å…ˆéªŒè¯APIå¯†é’¥
                if not validate_api_key(VOLC_API_KEY, "volc"):
                    st.error("âŒ ç«å±±å¼•æ“APIå¯†é’¥æ ¼å¼æ— æ•ˆ")
                    return
                if not validate_api_key(DEEPSEEK_API_KEY, "deepseek"):
                    st.error("âŒ DeepSeek APIå¯†é’¥æ ¼å¼æ— æ•ˆ")
                    return
                
                status_text.text("2. æµ‹è¯•åµŒå…¥åŠŸèƒ½...")
                progress.progress(0.4)
                time.sleep(0.5)
                
                # æµ‹è¯•åµŒå…¥åŠŸèƒ½
                test_embeddings = VolcEngineEmbeddings(api_key=VOLC_API_KEY)
                test_result = test_embeddings.embed_documents(["æµ‹è¯•æ–‡æœ¬"])
                
                if test_result:
                    st.success("âœ… åµŒå…¥åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
                    
                    status_text.text("3. æ£€æŸ¥å‘é‡æ•°æ®åº“...")
                    progress.progress(0.6)
                    time.sleep(0.5)
                    
                    # å¦‚æœæœ‰å‘é‡æ•°æ®åº“ï¼Œæµ‹è¯•æ£€ç´¢åŠŸèƒ½
                    if os.path.exists(CHROMA_DB_PATH) and os.listdir(CHROMA_DB_PATH):
                        status_text.text("4. æµ‹è¯•å‘é‡æ•°æ®åº“æ£€ç´¢...")
                        progress.progress(0.8)
                        time.sleep(0.5)
                        
                        success, vector_store = safe_execute(load_vector_store, test_embeddings, CHROMA_DB_PATH)
                        if success:
                            vector_store, message = vector_store  # è§£åŒ…è¿”å›å€¼
                            if vector_store:
                                test_docs, search_message = search_vector_store(vector_store, "æµ‹è¯•", k=1)
                                st.success("âœ… å‘é‡æ•°æ®åº“æ£€ç´¢åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
                                st.info(message)
                                progress.progress(1.0)
                                status_text.text("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
                            else:
                                st.warning(f"âš ï¸ {message}")
                                progress.progress(0.9)
                        else:
                            st.error(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {vector_store}")
                    else:
                        st.info("â„¹ï¸ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶")
                        progress.progress(1.0)
                        status_text.text("âœ… APIæµ‹è¯•é€šè¿‡ï¼Œå‘é‡æ•°æ®åº“å°šæœªåˆ›å»º")
                else:
                    st.error("âŒ åµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥")
                    progress.progress(0)
            
            except Exception as e:
                error_msg = handle_error(e)
                st.error(f"âŒ æµ‹è¯•å¤±è´¥: {error_msg}")
                progress.progress(0)
            finally:
                # æ¸…ç†çŠ¶æ€æ–‡æœ¬
                time.sleep(1)
                status_text.empty()
                progress.empty()


def process_pdf_files(uploaded_files):
    """
    å¤„ç†ä¸Šä¼ çš„PDFæ–‡ä»¶
    
    Args:
        uploaded_files: Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡åˆ—è¡¨
    """
    if not uploaded_files:
        return
    
    # æ›´æ–°å¤„ç†çŠ¶æ€
    st.session_state.processing_status = "å¤„ç†PDFæ–‡ä»¶ä¸­"
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™äº›æ–‡ä»¶
    new_files = []
    for file in uploaded_files:
        if file.name not in st.session_state.uploaded_files:
            new_files.append(file)
    
    if not new_files:
        st.info("è¿™äº›æ–‡ä»¶å·²ç»ä¸Šä¼ è¿‡äº†")
        st.session_state.processing_status = "idle"
        return
    
    all_texts = []
    total_pages = 0
    total_tokens = 0
    
    # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_idx, file in enumerate(new_files):
            status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {file_idx + 1}/{len(new_files)}: {file.name}")
            progress_bar.progress((file_idx + 1) / (len(new_files) + 1))  # ç•™ä¸€ç‚¹ç»™çŸ¥è¯†åº“åˆ›å»º
            
            # æå–æ–‡æœ¬
            text, message = extract_text_from_pdf(file)
            
            if not text.strip():
                st.warning(f"å¤„ç†æ–‡ä»¶ {file.name} å¤±è´¥: {message}")
                continue
            
            # æ¸…ç†æ–‡æœ¬
            text = clean_text(text)
            all_texts.append(text)
            
            # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶
            st.session_state.uploaded_files.append(file.name)
            
            # ä¼°ç®—tokenæ•°é‡
            token_count = estimate_token_count(text)
            total_tokens += token_count
            
            # ä»æ¶ˆæ¯ä¸­æå–é¡µæ•°
            if "æˆåŠŸæå–" in message:
                import re
                pages_match = re.search(r'æˆåŠŸæå– (\d+) é¡µ', message)
                if pages_match:
                    total_pages += int(pages_match.group(1))
        
        if not all_texts:
            st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•PDFæ–‡ä»¶")
            st.session_state.processing_status = "idle"
            return
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = "\n\n".join(all_texts)
        st.success(f"âœ… æˆåŠŸå¤„ç† {len(new_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_pages} é¡µï¼Œçº¦ {total_tokens} tokens")
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        create_knowledge_base(combined_text)
        
    except Exception as e:
        error_msg = handle_error(e)
        st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {error_msg}")
        st.session_state.last_error = error_msg
    finally:
        # æ¸…ç†è¿›åº¦æ¡å’ŒçŠ¶æ€
        progress_bar.empty()
        status_text.empty()
        st.session_state.processing_status = "idle"


def create_knowledge_base(text):
    """
    åˆ›å»ºçŸ¥è¯†åº“
    
    Args:
        text: åˆå¹¶åçš„æ–‡æœ¬å†…å®¹
    """
    st.session_state.processing_status = "åˆ›å»ºçŸ¥è¯†åº“ä¸­"
    
    try:
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = VolcEngineEmbeddings(api_key=VOLC_API_KEY)
        
        # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡å‰²
        status_text.text("æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        progress_bar.progress(0.1)
        chunks = create_text_chunks(text, CHUNK_SIZE, CHUNK_OVERLAP)
        st.info(f"æ–‡æœ¬è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªç‰‡æ®µ")
        
        # æµ‹è¯•åµŒå…¥åŠŸèƒ½
        status_text.text("æ­£åœ¨æµ‹è¯•åµŒå…¥åŠŸèƒ½...")
        progress_bar.progress(0.2)
        test_chunks = chunks[:2] if len(chunks) >= 2 else chunks
        test_embeddings_result = embeddings.embed_documents(test_chunks)
        
        if not test_embeddings_result:
            st.error("åµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•åˆ›å»ºå‘é‡æ•°æ®åº“")
            progress_bar.empty()
            status_text.empty()
            st.session_state.processing_status = "idle"
            return
        
        status_text.text("åµŒå…¥åŠŸèƒ½æµ‹è¯•æˆåŠŸï¼Œæ­£åœ¨å‡†å¤‡æ–‡æ¡£å…ƒæ•°æ®...")
        progress_bar.progress(0.3)
        
        # æ·»åŠ æ–‡æ¡£å…ƒæ•°æ®
        metadatas = []
        # åˆ†æ‰¹ç”Ÿæˆå…ƒæ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†è¿‡å¤š
        total_chunks = len(chunks)
        for i in range(0, total_chunks, 1000):  # æ¯1000ä¸ªå—ä¸€æ‰¹
            end_idx = min(i + 1000, total_chunks)
            for j in range(i, end_idx):
                metadatas.append({
                    "source": f"chunk_{j}", 
                    "timestamp": datetime.now().isoformat(),
                    "chunk_size": len(chunks[j]),
                    "chunk_id": j
                })
            # æ›´æ–°è¿›åº¦
            progress_bar.progress(min(0.3 + (0.1 * (end_idx / total_chunks)), 0.4))
        
        status_text.text("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        progress_bar.progress(0.4)
        
        # æ ¹æ®å—æ•°é‡åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        total_chunks = len(chunks)
        
        # å°æ–‡ä»¶ç›´æ¥å¤„ç†
        if total_chunks <= 100:
            # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å—
            vector_store, message = create_vector_store(
                texts=chunks,
                embeddings=embeddings,
                persist_directory=CHROMA_DB_PATH,
                metadatas=metadatas,
                neo4j_uri=NEO4J_URI,
                neo4j_user=NEO4J_USER,
                neo4j_password=NEO4J_PASSWORD
            )
            
            if not vector_store:
                st.error(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {message}")
                progress_bar.empty()
                status_text.empty()
                st.session_state.processing_status = "idle"
                return
        else:
            # å¤§æ–‡ä»¶åˆ†æ‰¹æ¬¡å¤„ç†ï¼Œä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
            st.info(f"æ–‡ä»¶è¾ƒå¤§ï¼ˆ{total_chunks}ä¸ªç‰‡æ®µï¼‰ï¼Œæ­£åœ¨åˆ†æ‰¹æ¬¡å¤„ç†...")
            
            # æ ¹æ®å—æ•°é‡åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
            if total_chunks <= 500:
                batch_size = 50
            elif total_chunks <= 2000:
                batch_size = 100
            else:
                batch_size = 200
            
            total_batches = (total_chunks + batch_size - 1) // batch_size
            
            # ç¡®ä¿å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨
            if not os.path.exists(CHROMA_DB_PATH):
                os.makedirs(CHROMA_DB_PATH)
            
            # å…ˆåˆ›å»ºç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            first_batch_chunks = chunks[:batch_size]
            first_batch_metadatas = metadatas[:batch_size]
            
            status_text.text(f"åˆ›å»ºç¬¬ä¸€æ‰¹å‘é‡ ({batch_size}/{total_chunks})...")
            progress_bar.progress(0.4)
            
            vector_store, message = create_vector_store(
                texts=first_batch_chunks,
                embeddings=embeddings,
                persist_directory=CHROMA_DB_PATH,
                metadatas=first_batch_metadatas,
                neo4j_uri=NEO4J_URI,
                neo4j_user=NEO4J_USER,
                neo4j_password=NEO4J_PASSWORD
            )
            
            if not vector_store:
                st.error(f"âŒ åˆ›å»ºç¬¬ä¸€æ‰¹å‘é‡å¤±è´¥: {message}")
                progress_bar.empty()
                status_text.empty()
                st.session_state.processing_status = "idle"
                return
            
            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            for i in range(1, total_batches):
                start_idx = i * batch_size
                end_idx = min(start_idx + batch_size, total_chunks)
                
                # æ›´æ–°çŠ¶æ€æ–‡æœ¬ï¼Œé¿å…é¢‘ç¹æ›´æ–°
                if i % 5 == 0 or end_idx >= total_chunks:  # æ¯5æ‰¹æˆ–æœ€åä¸€æ‰¹æ—¶æ›´æ–°
                    status_text.text(f"æ·»åŠ æ‰¹æ¬¡ {i+1}/{total_batches} ({end_idx}/{total_chunks})...")
                
                # æ›´æ–°è¿›åº¦æ¡ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡
                progress = 0.4 + (0.5 * i / total_batches)  # 0.4åˆ°0.9ä¹‹é—´çš„è¿›åº¦
                progress_bar.progress(progress)
                
                batch_chunks = chunks[start_idx:end_idx]
                batch_metadatas = metadatas[start_idx:end_idx]
                
                # å‘ç°æœ‰å‘é‡å­˜å‚¨æ·»åŠ æ–°æ–‡æ¡£
                try:
                    # æ·»åŠ æ–‡æ¡£
                    vector_store.add_texts(
                        texts=batch_chunks,
                        metadatas=batch_metadatas
                    )
                    
                    # æ¯å¤„ç†3æ‰¹æ‰æŒä¹…åŒ–ä¸€æ¬¡ï¼Œå‡å°‘I/Oæ“ä½œ
                    if i % 3 == 0 or end_idx >= total_chunks:
                        vector_store.persist()
                        status_text.text(f"æ‰¹æ¬¡ {i+1}/{total_batches} å·²æŒä¹…åŒ–...")
                except Exception as e:
                    # å°è¯•é‡æ–°åŠ è½½å‘é‡å­˜å‚¨åé‡è¯•
                    st.warning(f"âš ï¸ æ·»åŠ æ‰¹æ¬¡å¤±è´¥ï¼Œå°è¯•é‡è¯•: {str(e)}")
                    # å…ˆæŒä¹…åŒ–å½“å‰çŠ¶æ€
                    try:
                        vector_store.persist()
                    except:
                        pass
                    
                    # é‡æ–°åŠ è½½å‘é‡å­˜å‚¨
                    try:
                        from utils.vector_utils import load_vector_store
                        vector_store, _ = load_vector_store(embeddings, CHROMA_DB_PATH)
                        if vector_store:
                            # é‡è¯•æ·»åŠ å½“å‰æ‰¹æ¬¡
                            vector_store.add_texts(
                                texts=batch_chunks,
                                metadatas=batch_metadatas
                            )
                            vector_store.persist()
                            st.info(f"âœ… æ‰¹æ¬¡ {i+1} é‡è¯•æˆåŠŸ")
                        else:
                            raise Exception("æ— æ³•é‡æ–°åŠ è½½å‘é‡å­˜å‚¨")
                    except Exception as retry_error:
                        st.error(f"âŒ æ·»åŠ æ‰¹æ¬¡å¤±è´¥ï¼Œé‡è¯•ä¹ŸæœªæˆåŠŸ: {str(retry_error)}")
                        progress_bar.empty()
                        status_text.empty()
                        st.session_state.processing_status = "idle"
                        return
            vector_store, message = create_vector_store(
                texts=chunks,
                embeddings=embeddings,
                persist_directory=CHROMA_DB_PATH,
                metadatas=metadatas
            )
        
        status_text.text("å®ŒæˆçŸ¥è¯†åº“åˆ›å»º...")
        progress_bar.progress(0.95)
        
        if vector_store:
            vector_store.persist()  # ç¡®ä¿æ•°æ®æŒä¹…åŒ–
            st.session_state.vector_store = vector_store
            st.session_state.pdf_processed = True
            st.session_state.total_chunks = len(chunks)
            st.success(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼åŒ…å« {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            st.balloons()
        else:
            st.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {message}")
            
    except Exception as e:
        error_msg = handle_error(e)
        st.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {error_msg}")
        st.session_state.last_error = error_msg
    finally:
        # æ¸…ç†è¿›åº¦æ¡å’ŒçŠ¶æ€
        if 'progress_bar' in locals():
            progress_bar.empty()
        if 'status_text' in locals():
            status_text.empty()
        st.session_state.processing_status = "idle"


def load_existing_knowledge_base():
    """
    åŠ è½½ç°æœ‰çš„çŸ¥è¯†åº“
    """
    try:
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(CHROMA_DB_PATH) or not os.listdir(CHROMA_DB_PATH):
            log_info("å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
            return
        
        embeddings = VolcEngineEmbeddings(api_key=VOLC_API_KEY)
        vector_store, message = load_vector_store(embeddings, CHROMA_DB_PATH)
        
        if vector_store:
            st.session_state.vector_store = vector_store
            # è·å–æ–‡æ¡£æ•°é‡
            try:
                st.session_state.total_chunks = vector_store._collection.count()
                log_info(f"æˆåŠŸåŠ è½½ç°æœ‰çŸ¥è¯†åº“: {message}")
                st.success("âœ… å·²åŠ è½½ç°æœ‰çŸ¥è¯†åº“")
                
                # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
                try:
                    from utils.retrieval_utils import initialize_hybrid_retriever
                    # ä»å‘é‡å­˜å‚¨ä¸­è·å–æ‰€æœ‰æ–‡æ¡£å¹¶æ·»åŠ åˆ°æ··åˆæ£€ç´¢å™¨
                    
                    # å‡†å¤‡Neo4jé…ç½®
                    neo4j_config = None
                    if NEO4J_URI and NEO4J_USER and NEO4J_PASSWORD:
                        neo4j_config = {
                            'uri': NEO4J_URI,
                            'user': NEO4J_USER,
                            'password': NEO4J_PASSWORD
                        }
                        log_info("Neo4jé…ç½®å·²æä¾›ï¼Œå°†ä½¿ç”¨Neo4jçŸ¥è¯†å›¾è°±")
                    
                    # è·å–å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£
                    try:
                        # å°è¯•è·å–æ‰€æœ‰æ–‡æ¡£
                        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šè·å–å¤§é‡æ–‡æ¡£ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦é™åˆ¶æ•°é‡
                        documents = []
                        log_info("æ­£åœ¨ä»å‘é‡å­˜å‚¨ä¸­æå–æ–‡æ¡£")
                        
                        # è·å–æ–‡æ¡£æ•°é‡
                        doc_count = vector_store._collection.count()
                        log_info(f"å‘é‡å­˜å‚¨ä¸­æœ‰ {doc_count} ä¸ªæ–‡æ¡£")
                        
                        # åˆ†æ‰¹è·å–æ–‡æ¡£ï¼ˆé¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šï¼‰
                        batch_size = 1000
                        for i in range(0, doc_count, batch_size):
                            try:
                                # è·å–æ‰¹æ¬¡æ–‡æ¡£
                                results = vector_store.similarity_search("", k=min(batch_size, doc_count - i))
                                documents.extend(results)
                                log_info(f"å·²æå– {len(documents)}/{doc_count} ä¸ªæ–‡æ¡£")
                            except Exception as batch_e:
                                log_warning(f"æå–æ–‡æ¡£æ‰¹æ¬¡å¤±è´¥: {str(batch_e)}")
                                break
                        
                        log_info(f"æˆåŠŸæå– {len(documents)} ä¸ªæ–‡æ¡£")
                    except Exception as doc_e:
                        log_warning(f"è·å–æ–‡æ¡£å¤±è´¥: {str(doc_e)}")
                        documents = []
                    
                    # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
                    retriever = initialize_hybrid_retriever(vector_store, documents=documents, neo4j_config=neo4j_config)
                    log_info("æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
                except Exception as hybrid_e:
                    log_warning(f"åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨å¤±è´¥: {str(hybrid_e)}")
                    
            except Exception as inner_e:
                log_error(f"è·å–çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡å¤±è´¥: {str(inner_e)}")
    except Exception as e:
        log_error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}", exc_info=True)


def handle_question(query):
    """
    å¤„ç†ç”¨æˆ·æé—®
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
    """
    # ä»ä¼šè¯çŠ¶æ€è·å–æ£€ç´¢é…ç½®
    search_config = st.session_state.get('search_config', {
        'use_hybrid': True,
        'vector_weight': 0.4,
        'keyword_weight': 0.3,
        'kg_weight': 0.3
    })
    try:
        if not query:
            st.warning("è¯·è¾“å…¥é—®é¢˜")
            return
        
        # è®°å½•æŸ¥è¯¢æ—¶é—´
        st.session_state.last_query_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.session_state.processing_status = "å¤„ç†é—®é¢˜ä¸­"
        
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.subheader("é—®é¢˜ï¼š")
        st.write(query)
        
        # å¦‚æœæœ‰å‘é‡æ•°æ®åº“ï¼ˆå·²ä¸Šä¼ PDFæˆ–å·²å­˜åœ¨ï¼‰ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
        if st.session_state.vector_store:
            with st.spinner("æ­£åœ¨ä»çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯..."):
                try:
                    # ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ï¼‰
                    search_params = {
                        'vector_store': st.session_state.vector_store,
                        'query': query,
                        'k': st.session_state.get('search_k', 3),  # é»˜è®¤å€¼ä¸º3
                        'use_hybrid': search_config['use_hybrid']
                    }
                    
                    # å¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼Œæ·»åŠ æƒé‡å‚æ•°
                    if search_config['use_hybrid'] and 'vector_weight' in search_config:
                        search_params['weights'] = {
                            'vector_weight': search_config['vector_weight'],
                            'keyword_weight': search_config['keyword_weight'],
                            'kg_weight': search_config['kg_weight']
                        }
                    
                    relevant_docs, search_message = search_vector_store(**search_params)
                    log_info(f"æ£€ç´¢é…ç½®: æ··åˆæ£€ç´¢={search_config['use_hybrid']}, ç»“æœæ•°é‡={len(relevant_docs)}")
                    
                    if not relevant_docs:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä½¿ç”¨é€šç”¨æ¨¡å‹å›ç­”")
                        # é™çº§ä¸ºé€šç”¨æ¨¡å‹å›ç­”
                        generate_answer_with_general_model(query)
                        return
                    
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in relevant_docs])
                    
                    # è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œé’ˆå¯¹æ³•å¾‹é¢†åŸŸä¼˜åŒ–
                    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹çŸ¥è¯†åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæä¾›çš„æ³•å¾‹æ–‡æ¡£å›ç­”é—®é¢˜ã€‚è¯·ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¿æŒä¸“ä¸šã€å‡†ç¡®ã€‚"
                    
                    # åˆ›å»ºæç¤ºè¯
                    prompt = create_rag_prompt(context, query, system_prompt)
                    
                    # è°ƒç”¨DeepSeekæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
                    llm = DeepSeekLLM(
                        api_key=DEEPSEEK_API_KEY,
                        api_base=DEEPSEEK_API_URL,
                        model_name=LLM_MODEL,
                        temperature=TEMPERATURE
                    )
                    
                    st.info("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
                    answer = llm.invoke(prompt)
                    
                    # æ˜¾ç¤ºç­”æ¡ˆ
                    st.subheader("å›ç­”ï¼ˆåŸºäºçŸ¥è¯†åº“ï¼‰ï¼š")
                    st.write(answer)
                    
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
                    with st.expander("æŸ¥çœ‹æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹"):
                        for i, doc in enumerate(relevant_docs):
                            st.markdown(f"**ç›¸å…³ç‰‡æ®µ {i+1}:**")
                            st.write(doc.page_content)
                            if hasattr(doc, 'metadata') and doc.metadata:
                                st.caption(f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
                            st.divider()
                    
                    # ä¿å­˜åˆ°å†å²è®°å½•
                    st.session_state.chat_history.append({
                        "query": query,
                        "answer": answer,
                        "timestamp": st.session_state.last_query_time,
                        "source": "çŸ¥è¯†åº“",
                        "context_length": len(context)
                    })
                    
                except Exception as e:
                    error_msg = handle_error(e)
                    st.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {error_msg}")
                    st.session_state.last_error = error_msg
                    # é™çº§ä¸ºé€šç”¨æ¨¡å‹
                    st.info("å°è¯•ä½¿ç”¨é€šç”¨æ¨¡å‹å›ç­”...")
                    generate_answer_with_general_model(query)
        else:
            # æ²¡æœ‰çŸ¥è¯†åº“ï¼Œä½¿ç”¨é€šç”¨æ¨¡å‹å›ç­”
            generate_answer_with_general_model(query)
    finally:
        st.session_state.processing_status = "idle"


def generate_answer_with_general_model(query):
    """
    ä½¿ç”¨é€šç”¨æ¨¡å‹å›ç­”é—®é¢˜ï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“ï¼‰
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
    """
    with st.spinner("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
        try:
            # éªŒè¯DeepSeek APIå¯†é’¥
            if not validate_api_key(DEEPSEEK_API_KEY, "deepseek"):
                st.error("DeepSeek APIå¯†é’¥æ— æ•ˆ")
                return
            
            # åˆ›å»ºé€šç”¨æç¤ºè¯ï¼Œé’ˆå¯¹æ³•å¾‹é¢†åŸŸä¼˜åŒ–
            custom_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹æ³•å¾‹é—®é¢˜æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚

é—®é¢˜ï¼š
{query}

è¯·æä¾›ä¸“ä¸šã€å®¢è§‚çš„æ³•å¾‹åˆ†æï¼š
"""
            
            # è°ƒç”¨DeepSeekæ¨¡å‹
            llm = DeepSeekLLM(
                api_key=DEEPSEEK_API_KEY,
                api_base=DEEPSEEK_API_URL,
                model_name=LLM_MODEL,
                temperature=TEMPERATURE,
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            answer = llm.invoke(custom_prompt)
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            st.subheader("å›ç­”ï¼ˆé€šç”¨æ¨¡å¼ï¼‰ï¼š")
            st.write(answer)
            st.info("ğŸ’¡ æç¤ºï¼šä¸Šä¼ PDFæ–‡ä»¶å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„åŸºäºæ–‡æ¡£çš„å›ç­”")
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            st.session_state.chat_history.append({
                "query": query,
                "answer": answer,
                "timestamp": st.session_state.last_query_time,
                "source": "é€šç”¨æ¨¡å‹"
            })
            
        except requests.exceptions.Timeout:
            st.error("ç”Ÿæˆç­”æ¡ˆè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
            st.session_state.last_error = "ç”Ÿæˆç­”æ¡ˆè¶…æ—¶"
        except requests.exceptions.ConnectionError:
            st.error("ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
            st.session_state.last_error = "ç½‘ç»œè¿æ¥é”™è¯¯"
        except Exception as e:
            error_msg = handle_error(e)
            st.error(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {error_msg}")
            st.session_state.last_error = error_msg


def display_chat_history():
    """
    æ˜¾ç¤ºèŠå¤©å†å²
    """
    if st.session_state.chat_history:
        with st.expander("æŸ¥çœ‹å†å²è®°å½•", expanded=False):
            # æ·»åŠ æ¸…é™¤å†å²è®°å½•æŒ‰é’®
            if st.button("æ¸…ç©ºå†å²è®°å½•", key="clear_history"):
                st.session_state.chat_history = []
                st.success("å†å²è®°å½•å·²æ¸…ç©º")
                st.rerun()
            
            # åˆ†é¡µæ˜¾ç¤ºå†å²è®°å½•
            page_size = 5
            total_pages = (len(st.session_state.chat_history) + page_size - 1) // page_size
            
            if total_pages > 1:
                page = st.selectbox("é€‰æ‹©é¡µç ", range(1, total_pages + 1), key="history_page")
                start_idx = (page - 1) * page_size
                end_idx = min(start_idx + page_size, len(st.session_state.chat_history))
                display_chats = reversed(st.session_state.chat_history[start_idx:end_idx])
            else:
                display_chats = reversed(st.session_state.chat_history)
            
            # æ˜¾ç¤ºèŠå¤©è®°å½•
            for idx, chat in enumerate(display_chats):
                st.markdown(f"**ğŸ“ [{chat['timestamp']}] ç”¨æˆ·é—®é¢˜:**")
                st.write(chat['query'])
                
                st.markdown(f"**ğŸ’¡ [{chat['timestamp']}] å›ç­” ({chat['source']}):**")
                st.write(chat['answer'])
                
                # æ·»åŠ é‡æ–°æé—®æŒ‰é’®
                if st.button(f"é‡æ–°æé—®", key=f"reask_{idx}"):
                    # è¿™é‡Œå¯ä»¥å®ç°é‡æ–°æé—®çš„é€»è¾‘
                    st.info(f"å·²å¤åˆ¶é—®é¢˜: {chat['query']}")
                
                st.divider()
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            st.caption(f"å…± {len(st.session_state.chat_history)} æ¡å†å²è®°å½•")


def check_config():
    """
    æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
    """
    missing = []
    
    # æ£€æŸ¥å…³é”®é…ç½®é¡¹
    if not VOLC_API_KEY:
        missing.append("VOLC_API_KEY")
    if not DEEPSEEK_API_KEY:
        missing.append("DEEPSEEK_API_KEY")
    if not DEEPSEEK_API_URL:
        missing.append("DEEPSEEK_API_URL")
    
    return missing

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # æ£€æŸ¥é…ç½®
        missing_configs = check_config()
        if missing_configs:
            st.error(f"ç¼ºå°‘å¿…è¦çš„é…ç½®: {', '.join(missing_configs)}")
            st.info("è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„é…ç½®")
            
            # æ˜¾ç¤ºé…ç½®æŒ‡å¯¼
            with st.expander("é…ç½®ç¤ºä¾‹", expanded=True):
                st.code("""
# .env æ–‡ä»¶ç¤ºä¾‹
VOLC_API_KEY=your_volc_api_key_here
EMBEDDING_MODEL=ep-m-20250718174411-j9zsb
DEEPSEEK_API_KEY=sk-your_deepseek_api_key
DEEPSEEK_API_URL=https://api.deepseek.com/v1
LLM_MODEL=deepseek-chat
TEMPERATURE=0.1
CHROMA_DB_PATH=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50
                """)
            return
    
        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        initialize_session_state()
    
        # å°è¯•åŠ è½½ç°æœ‰çš„çŸ¥è¯†åº“
        if not st.session_state.vector_store:
            load_existing_knowledge_base()
    
        # è®¾ç½®UI
        uploaded_files = setup_streamlit_ui()
    
        # å¤„ç†PDFä¸Šä¼ 
        if uploaded_files:
            process_pdf_files(uploaded_files)
    
        # ç”¨æˆ·æé—®åŒºåŸŸ
        st.divider()
        st.subheader("ğŸ’¬ æé—®")
    
        # ä½¿ç”¨æ–‡æœ¬åŒºåŸŸæ›¿ä»£å•è¡Œè¾“å…¥æ¡†
        query = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", 
            placeholder="ä¾‹å¦‚ï¼šå¤«å¦»ç¦»å©šæ—¶ï¼Œå©šå‰æˆ¿äº§åº”è¯¥å¦‚ä½•å¤„ç†?",
            height=120,
            key="query_input"
        )
    
        # æ·»åŠ é«˜çº§é€‰é¡¹
        with st.expander("é«˜çº§é€‰é¡¹", expanded=False):
            st.session_state.search_k = st.slider("æœç´¢ç»“æœæ•°é‡", 1, 5, 3, help="ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢çš„ç›¸å…³æ–‡æ¡£æ•°é‡")
            temperature = st.slider("ç”Ÿæˆæ¸©åº¦", 0.0, 1.0, TEMPERATURE, 0.1, help="æ§åˆ¶ç”Ÿæˆç­”æ¡ˆçš„éšæœºæ€§")
            
            # æ·»åŠ æ£€ç´¢é…ç½®é€‰é¡¹
            st.markdown("### æ£€ç´¢é…ç½®")
            use_hybrid = st.checkbox("å¯ç”¨æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯+çŸ¥è¯†å›¾è°±ï¼‰", value=True, key="use_hybrid")
            
            if use_hybrid:
                st.caption("è°ƒæ•´å„æ£€ç´¢ç»„ä»¶çš„æƒé‡ï¼ˆæ€»å’Œå»ºè®®ä¸º1.0ï¼‰")
                col1, col2, col3 = st.columns(3)
                with col1:
                    vector_weight = st.slider("å‘é‡æ£€ç´¢æƒé‡", 0.0, 1.0, 0.4, 0.1, key="vector_weight")
                with col2:
                    keyword_weight = st.slider("å…³é”®è¯æ£€ç´¢æƒé‡", 0.0, 1.0, 0.3, 0.1, key="keyword_weight")
                with col3:
                    kg_weight = st.slider("çŸ¥è¯†å›¾è°±æƒé‡", 0.0, 1.0, 0.3, 0.1, key="kg_weight")
                
                # ä¿å­˜é…ç½®åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.search_config = {
                    'use_hybrid': use_hybrid,
                    'vector_weight': vector_weight,
                    'keyword_weight': keyword_weight,
                    'kg_weight': kg_weight
                }
            else:
                st.session_state.search_config = {'use_hybrid': use_hybrid}
            
            # æ·»åŠ é¢„è®¾é—®é¢˜æŒ‰é’®
            st.markdown("### é¢„è®¾é—®é¢˜")
            preset_questions = [
                "å©šå‰è´¢äº§å¦‚ä½•ç•Œå®šï¼Ÿ",
                "ç¦»å©šæ—¶å­å¥³æŠšå…»æƒå¦‚ä½•åˆ¤å®šï¼Ÿ",
                "å¤«å¦»å…±åŒå€ºåŠ¡å¦‚ä½•å¤„ç†ï¼Ÿ",
                "é—äº§ç»§æ‰¿çš„é¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿ"
            ]
            
            cols = st.columns(2)
            for i, q in enumerate(preset_questions):
                if cols[i % 2].button(q, key=f"preset_{i}"):
                    # å°†é¢„è®¾é—®é¢˜å¡«å……åˆ°è¾“å…¥æ¡†ï¼ˆè¿™é‡Œé€šè¿‡çŠ¶æ€ç®¡ç†å®ç°ï¼‰
                    st.session_state.preset_query = q
                    st.rerun()
    
        # å¦‚æœæœ‰é¢„è®¾é—®é¢˜ï¼Œå¡«å……åˆ°è¾“å…¥æ¡†
        if 'preset_query' in st.session_state:
            query = st.text_area(
                "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", 
                value=st.session_state.preset_query,
                height=120,
                key="query_input_preset"
            )
            del st.session_state.preset_query  # æ¸…é™¤é¢„è®¾é—®é¢˜
    
        # æäº¤æŒ‰é’®
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            submit_button = st.button(
                "ğŸš€ æäº¤é—®é¢˜", 
                type="primary",
                use_container_width=True
            )
    
        if submit_button:
            handle_question(query)
    
        # æ˜¾ç¤ºèŠå¤©å†å²
        display_chat_history()
    
    except Exception as e:
        # æ•è·å…¨å±€å¼‚å¸¸
        error_msg = handle_error(e, show_traceback=True)
        st.error(f"åº”ç”¨å‘ç”Ÿé”™è¯¯: {error_msg}")
        st.session_state.last_error = error_msg


# ç¨‹åºå…¥å£ç‚¹
if __name__ == '__main__':
    main()
